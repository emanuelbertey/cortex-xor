/*
# mLSTM: Matrix Long Short-Term Memory

This module implements the mLSTM (matrix LSTM) cell and layer as described in the paper:
"xLSTM: Extended Long Short-Term Memory" by Beck et al. (2024).

The mLSTM extends the traditional LSTM by using a matrix memory state and exponential gating,
allowing for enhanced storage capacities and improved performance on long-range dependencies.
*/

use candle_core::{Tensor, Device, Result, DType};
use candle_nn::{Dropout, Module, VarBuilder, Linear, LayerNorm, ops, linear_no_bias, layer_norm};
use serde::{Deserialize, Serialize};

/// State for mLSTM containing cell matrix and hidden state
#[derive(Clone, Debug)]
pub struct MLstmstate {
    /// Cell state - matrix of shape [`batch_size`, `num_heads`, `head_dim`, `head_dim`]
    pub cell: Tensor,
    /// Hidden state - vector of shape [`batch_size`, `hidden_size`]
    pub hidden: Tensor,
    /// Normalizer state - vector of shape [`batch_size`, `num_heads`, `head_dim`]
    pub normalizer: Tensor,
    /// Global max gate state for numeric stability - shape [`batch_size`, `num_heads`, 1]
    pub max_gate_log: Tensor,
}

impl MLstmstate {
    /// Create a new mLSTM state
    pub fn new(
        cell: Tensor,
        hidden: Tensor,
        normalizer: Tensor,
        max_gate_log: Tensor,
    ) -> Self {
        Self {
            cell,
            hidden,
            normalizer,
            max_gate_log,
        }
    }

    pub fn detach(&self) -> Self {
        Self {
            cell: self.cell.detach(),
            hidden: self.hidden.detach(),
            normalizer: self.normalizer.detach(),
            max_gate_log: self.max_gate_log.detach(),
        }
    }
}

/// Configuration for mLSTM con todas las variables necesarias
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MLstmconfig {
    /// Size of input features
    pub d_input: usize,
    /// Size of hidden state
    pub d_hidden: usize,
    /// Number of layers
    pub num_layers: usize,
    /// Number of heads for multi-head mLSTM
    pub num_heads: usize,
    /// Expansion factor for inner dimension
    pub expansion_factor: usize,
    /// Dropout probability
    pub dropout: f32,
    /// Weight initialization standard deviation
    pub weight_stdev: f64,
    /// Forget gate bias (sigmoid)
    pub forget_bias: f32,
    /// Input gate bias (exponential)
    pub input_gate_bias: f32,
    /// Output gate bias (sigmoid)
    pub output_gate_bias: f32,
    /// Epsilon for numerical stability
    pub epsilon: f64,
    /// Log clamp value
    pub log_clamp: f64,
    /// Exp clamp value
    pub exp_clamp: f64,
    /// Normalizer minimum clamp
    pub norm_clamp_min: f64,
    /// Normalizer maximum clamp
    pub norm_clamp_max: f64,
    /// Whether to use separate biases
    pub use_separate_bias: bool,
    /// Exponential gate scale
    pub exp_gate_scale: f64,
}

impl MLstmconfig {
    pub fn new(d_input: usize, d_hidden: usize, num_layers: usize, num_heads: usize) -> Self {
        assert!(
            d_hidden % num_heads == 0,
            "hidden_size debe ser divisible por num_heads"
        );
        
        Self {
            d_input,
            d_hidden,
            num_layers,
            num_heads,
            expansion_factor: 2,
            dropout: 0.0,
            weight_stdev: 0.02,
            forget_bias: 1.0,
            input_gate_bias: 0.0,
            output_gate_bias: 0.0,
            epsilon: 1e-6,
            log_clamp: 1e-8,
            exp_clamp: 15.0,
            norm_clamp_min: 1e-6,
            norm_clamp_max: 1e10,
            use_separate_bias: true,
            exp_gate_scale: 1.0,
        }
    }

    pub fn with_expansion_factor(mut self, factor: usize) -> Self {
        self.expansion_factor = factor;
        self
    }

    pub fn with_dropout(mut self, dropout: f32) -> Self {
        self.dropout = dropout;
        self
    }
    
    pub fn with_weight_stdev(mut self, stdev: f64) -> Self {
        self.weight_stdev = stdev;
        self
    }
    
    pub fn with_forget_bias(mut self, bias: f32) -> Self {
        self.forget_bias = bias;
        self
    }
    
    pub fn with_input_gate_bias(mut self, bias: f32) -> Self {
        self.input_gate_bias = bias;
        self
    }
    
    pub fn with_output_gate_bias(mut self, bias: f32) -> Self {
        self.output_gate_bias = bias;
        self
    }
    
    pub fn with_exp_gate_scale(mut self, scale: f64) -> Self {
        self.exp_gate_scale = scale;
        self
    }

    /// Initialize a new mLSTM
    pub fn init(&self, vb: VarBuilder) -> Result<MLstm> {
        let mut layers = Vec::with_capacity(self.num_layers);
        for i in 0..self.num_layers {
            let input_size = if i == 0 { self.d_input } else { self.d_hidden };
            let layer_vb = vb.pp(format!("layer_{}", i));
            layers.push(MLstmcell::new(
                input_size, 
                self.d_hidden, 
                self.num_heads, 
                self.expansion_factor,
                self, // Pasamos todo el config
                layer_vb
            )?);
        }

        Ok(MLstm {
            layers,
            dropout_layer: Dropout::new(self.dropout),
            d_input: self.d_input,
            d_hidden: self.d_hidden,
            num_layers: self.num_layers,
            num_heads: self.num_heads,
            expansion_factor: self.expansion_factor,
            dropout: self.dropout,
            config: self.clone(),
        })
    }
}

/// mLSTM layer implementation
#[derive(Debug)]
pub struct MLstm {
    /// Stack of mLSTM cells
    pub layers: Vec<MLstmcell>,
    /// Dropout module for inter-layer dropout
    pub dropout_layer: Dropout,
    /// Input size
    pub d_input: usize,
    /// Hidden size
    pub d_hidden: usize,
    /// Number of layers
    pub num_layers: usize,
    /// Number of heads
    pub num_heads: usize,
    /// Expansion factor
    pub expansion_factor: usize,
    /// Dropout probability
    pub dropout: f32,
    /// Full configuration
    pub config: MLstmconfig,
}

impl MLstm {
    /// Forward pass through mLSTM consuming and returning states
    pub fn forward(
        &self,
        input_seq: &Tensor,
        states: Option<Vec<MLstmstate>>,
    ) -> Result<(Tensor, Vec<MLstmstate>)> {
        let (batch_size, _seq_length, _) = input_seq.dims3()?;
        let device = input_seq.device();

        // Inicializar estados
        let mut hidden_states = match states {
            Some(s) => s,
            None => self.init_hidden(batch_size, device)?,
        };
        
        let mut layer_input = input_seq.clone();

        for (layer_idx, layer) in self.layers.iter().enumerate() {
            // mLSTM processes the entire sequence using the parallel kernel (Dual Form)
            let old_state = &hidden_states[layer_idx];
            
            // We pass the full sequence
            let (h_seq, new_state) = layer.forward_sequence(&layer_input, old_state)?;
            
            // Store the final state for future sequences (re-injecting continuity)
            hidden_states[layer_idx] = new_state;

            // Inter-layer Dropout
            layer_input = if layer_idx < self.num_layers - 1 && self.dropout > 0.0 {
                self.dropout_layer.forward(&h_seq, true)?
            } else {
                h_seq
            };
        }

        Ok((layer_input, hidden_states))
    }

    /// Initialize hidden states
    fn init_hidden(&self, batch_size: usize, device: &Device) -> Result<Vec<MLstmstate>> {
        let d_inner = self.d_hidden * self.expansion_factor;
        let head_dim = d_inner / self.num_heads;
        
        (0..self.num_layers)
            .map(|_| {
                Ok(MLstmstate::new(
                    Tensor::zeros((batch_size, self.num_heads, head_dim, head_dim), DType::F32, device)?,
                    Tensor::zeros((batch_size, self.d_hidden), DType::F32, device)?,
                    Tensor::zeros((batch_size, self.num_heads, head_dim), DType::F32, device)?,
                    Tensor::zeros((batch_size, self.num_heads, 1), DType::F32, device)?,
                ))
            })
            .collect()
    }
}

/// mLSTM cell implementation with matrix memory
#[derive(Debug)]
pub struct MLstmcell {
    /// Weight matrix for input to gates
    pub weight_ih: Tensor,
    /// Weight matrix for hidden to gates
    pub weight_hh: Tensor,
    /// Bias for gates (trainable base)
    pub bias: Tensor,
    /// Fixed bias offset to match Burn implementation (e.g. forget gate bias 0.5)
    pub bias_offset: Tensor,
    /// Query projection
    pub w_q: Linear,
    /// Key projection
    pub w_k: Linear,
    /// Value projection
    pub w_v: Linear,
    /// Down projection
    pub w_down: Linear,
    /// LayerNorm for multi-head normalization
    pub ln: LayerNorm,
    /// Input size
    pub input_size: usize,
    /// Hidden size
    pub hidden_size: usize,
    /// Number of heads
    pub num_heads: usize,
    /// Expansion factor
    pub expansion_factor: usize,
    /// Full configuration
    pub config: MLstmconfig,
}

impl MLstmcell {
    pub fn new(
        input_size: usize,
        hidden_size: usize,
        num_heads: usize,
        expansion_factor: usize,
        config: &MLstmconfig,
        vb: VarBuilder,
    ) -> Result<Self> {
        let d_inner = hidden_size * expansion_factor;
        
        let weight_init = candle_nn::init::Init::Randn {
            mean: 0.0,
            stdev: config.weight_stdev,
        };
        
        let weight_ih = vb.get_with_hints(
            (3 * d_inner, input_size), 
            "weight_ih", 
            weight_init.clone(),
        )?;
        
        let weight_hh = vb.get_with_hints(
            (3 * d_inner, hidden_size), 
            "weight_hh", 
            weight_init,
        )?;

        let bias = if config.use_separate_bias {
            let mut bias_vals = vec![0.0f32; 3 * d_inner];
            // Input gate bias (exponential)
            for i in (0 * d_inner)..(1 * d_inner) {
                bias_vals[i] = config.input_gate_bias;
            }
            // Forget gate bias (sigmoid) 
            for i in (1 * d_inner)..(2 * d_inner) {
                bias_vals[i] = config.forget_bias;
            }
            // Output gate bias (sigmoid)
            for i in (2 * d_inner)..(3 * d_inner) {
                bias_vals[i] = config.output_gate_bias;
            }
            Tensor::from_vec(bias_vals, (3 * d_inner,), vb.device())?
        } else {
            vb.get_with_hints(3 * d_inner, "bias", candle_nn::init::Init::Const(0.0))?
        };

        let device = vb.device();
        let mut b_offset_vals = vec![0.0f32; 3 * d_inner];
        for i in d_inner..(2 * d_inner) {
            b_offset_vals[i] = 0.5;
        }
        let bias_offset = Tensor::from_vec(b_offset_vals, (3 * d_inner,), device)?;

        let head_dim = d_inner / num_heads;

        // Q, K, V Proyectan a d_inner
        let w_q = linear_no_bias(input_size, d_inner, vb.pp("w_q"))?;
        let w_k = linear_no_bias(input_size, d_inner, vb.pp("w_k"))?;
        let w_v = linear_no_bias(input_size, d_inner, vb.pp("w_v"))?;
        
        // Down projection: d_inner -> hidden_size
        let w_down = linear_no_bias(d_inner, hidden_size, vb.pp("w_down"))?;
        
        let ln = layer_norm(head_dim, 1e-5, vb.pp("ln"))?;

        Ok(Self {
            weight_ih,
            weight_hh,
            bias,
            bias_offset,
            w_q,
            w_k,
            w_v,
            w_down,
            ln,
            input_size,
            hidden_size,
            num_heads,
            expansion_factor,
            config: config.clone(),
        })
    }

    /// Forward pass through mLSTM cell consuming the state
    pub fn forward_sequence(
        &self,
        input_seq: &Tensor,
        state: &MLstmstate,
    ) -> Result<(Tensor, MLstmstate)> {
        let (batch_size, seq_len, _) = input_seq.dims3()?;
        let d_inner = self.hidden_size * self.expansion_factor;
        let head_dim = d_inner / self.num_heads;
        let device = input_seq.device();

        // 1. Parallel Projections (Q, K, V)
        // input_seq: [B, S, D_in]
        // w_q(input): [B, S, D_h] -> reshape [B, S, H, D_head] -> transpose(1,2) -> [B, H, S, D_head]
        let q = self.w_q.forward(input_seq)?
            .reshape((batch_size, seq_len, self.num_heads, head_dim))?
            .permute((0, 2, 1, 3))? // [B, H, S, D_h]
            .contiguous()?;
        let k = self.w_k.forward(input_seq)?
            .reshape((batch_size, seq_len, self.num_heads, head_dim))?
            .permute((0, 2, 1, 3))?
            .contiguous()?;
        let v = self.w_v.forward(input_seq)?
            .reshape((batch_size, seq_len, self.num_heads, head_dim))?
            .permute((0, 2, 1, 3))?
            .contiguous()?;

        let scale = (head_dim as f64).sqrt();
        let q = (q / scale)?;
        let k = (k / scale)?;

        // 2. Parallel Gates
        let (batch_size, seq_len, _d_in) = input_seq.dims3()?;
        let input_flat = input_seq.reshape((batch_size * seq_len, self.input_size))?;
        
        let weight_ih_t = self.weight_ih.t()?.contiguous()?;
        let gates_flat = input_flat.matmul(&weight_ih_t)?;
        let gates = gates_flat.reshape((batch_size, seq_len, 3 * d_inner))?
            .broadcast_add(&self.bias.reshape((1, 1, 3 * d_inner))?)?
            .broadcast_add(&self.bias_offset.reshape((1, 1, 3 * d_inner))?)?;
        
        let chunks = gates.chunk(3, 2)?; // Chunk on last dim (dim 2)
        
        let i_raw = chunks[0].clone()
            .reshape((batch_size, seq_len, self.num_heads, head_dim))?
            .permute((0, 2, 1, 3))?;

        let f_raw = chunks[1].clone()
            .reshape((batch_size, seq_len, self.num_heads, head_dim))?
            .permute((0, 2, 1, 3))?;
        
        let o_pre = chunks[2].clone();
        
        // Apply exponential gate scaling and clamping
        let i_scaled = (i_raw / self.config.exp_gate_scale)?.clamp(-self.config.exp_clamp, self.config.exp_clamp)?;
        let i_gate = i_scaled.exp()?;
        
        let f_gate = ops::sigmoid(&f_raw)?;
        let o_gate = ops::sigmoid(&o_pre)?; // [B, S, D_hidden]

        // Logs for stability
        let i_log = i_gate.clamp(self.config.log_clamp as f32, f32::INFINITY)?.log()?;
        let f_log = f_gate.clamp(self.config.log_clamp as f32, 1.0)?.log()?;
        
        // Mean over head_dim
        let i_log_m = i_log.mean(3)?.unsqueeze(3)?; // [B, H, S, 1]
        let f_log_m = f_log.mean(3)?.unsqueeze(3)?; // [B, H, S, 1]

        // 3. Dual Form (Parallel Kernel)
        // Create causal mask
        let indices = Tensor::arange(0u32, seq_len as u32, device)?; 
        let row_idx = indices.reshape((seq_len, 1))?.broadcast_as((seq_len, seq_len))?;
        let col_idx = indices.reshape((1, seq_len))?.broadcast_as((seq_len, seq_len))?;
        let mask_tri = row_idx.ge(&col_idx)?.to_dtype(DType::F32)?; // 1 if row>=col (lower triangular)

        // f_cumsum = mask_tri @ f_log_m
        let f_cumsum = mask_tri.broadcast_as((batch_size, self.num_heads, seq_len, seq_len))?
            .matmul(&f_log_m)?; // [B, H, S, 1]

        // log_weights calculation
        let f_cumsum_t = f_cumsum.permute((0, 1, 3, 2))?; // [B, H, 1, S]
        let i_log_m_t = i_log_m.permute((0, 1, 3, 2))?;
        let log_weights = f_cumsum.broadcast_sub(&f_cumsum_t)?
            .broadcast_add(&i_log_m_t)?; // [B, H, S, S]

        // Mask future
        let mask_bool = mask_tri.to_dtype(DType::U8)?;
        let min_val = -1e10f32;
        let log_weights_masked = mask_bool.broadcast_as(log_weights.shape())?
            .where_cond(&log_weights, &Tensor::new(min_val, device)?.broadcast_as(log_weights.shape())?)?;

        // Global Max Stabilization (m_t)
        let m_0 = state.max_gate_log.reshape((batch_size, self.num_heads, 1, 1))?;
        let m_initial = f_cumsum.broadcast_add(&m_0)?; // [B, H, S, 1]
        
        let m_i_row = log_weights_masked.max(3)?.unsqueeze(3)?;
        let m_i = m_i_row.maximum(&m_initial)?;
        let m_i_stable = m_i.clamp(-self.config.exp_clamp as f32, self.config.exp_clamp as f32)?;
        
        let log_diff = log_weights_masked.broadcast_sub(&m_i_stable)?;
        let weights = log_diff.clamp(-20.0, 0.0)?.exp()?;

        // Parallel Hidden State
        let kt = k.permute((0, 1, 3, 2))?.contiguous()?; 
        let q_k_t = q.matmul(&kt)?; 
        
        let w_q_k_t = weights.broadcast_mul(&q_k_t)?;
        let h_parallel = w_q_k_t.matmul(&v.contiguous()?)?;

        // Initial State Contribution
        let initial_scale = f_cumsum
            .broadcast_add(&m_0)?
            .broadcast_sub(&m_i_stable)?
            .exp()?
            .clamp(0.0, 1e10)?; // [B, H, S, 1]
        
        let h_initial_base = q.matmul(&state.cell)?;
        let h_initial = h_initial_base.broadcast_mul(&initial_scale)?;
        let h_heads = (h_parallel + h_initial)?;

        // Normalizer
        let n_parallel = weights.matmul(&k.contiguous()?)?;
        let n_initial_base = state.normalizer.reshape((batch_size, self.num_heads, 1, head_dim))?
            .broadcast_as((batch_size, self.num_heads, seq_len, head_dim))?;
        let n_initial = n_initial_base.broadcast_mul(&initial_scale)?;
        
        let n_heads = (n_parallel + n_initial)?.clamp(self.config.norm_clamp_min as f32, f32::MAX)?; 
        let h_normalized = (h_heads / n_heads.clone())?;

        // LayerNorm
        let h_reshaped = h_normalized.permute((0, 2, 1, 3))?.contiguous()?; // [B, S, H, D_h]
        let h_ln = self.ln.forward(&h_reshaped)?;
       
        let h_combined_inner = h_ln.reshape((batch_size, seq_len, d_inner))?;
        
        // Apply output gate
        let h_inner_gated = (o_gate * h_combined_inner)?;
        
        // 4. Down Projection
        let h_seq = self.w_down.forward(&h_inner_gated)?;
        let h_seq = h_seq.clamp(-10.0, 10.0)?;

        // 5. Update State (Final T) 
        let last_idx = seq_len - 1; 
        
        let final_m = m_i_stable.narrow(2, last_idx, 1)?.reshape((batch_size, self.num_heads, 1))?; 
        let final_norm = n_heads.narrow(2, last_idx, 1)?.reshape((batch_size, self.num_heads, head_dim))?; 

        let last_initial_scale = initial_scale.narrow(2, last_idx, 1)?.reshape((batch_size, self.num_heads, 1, 1))?; 
        let final_cell_initial = state.cell.broadcast_mul(&last_initial_scale)?; 
        
        let last_row_weights = weights.narrow(2, last_idx, 1)?; // [B, H, 1, S] 
        
        let weight_permuted = last_row_weights.permute((0, 1, 3, 2))?.contiguous()?; // [B, H, S, 1] 
        let v_weighted = v.broadcast_mul(&weight_permuted)?; // [B, H, S, D] 
        
        let v_weighted_t = v_weighted.permute((0, 1, 3, 2))?.contiguous()?; // [B, H, D, S] 
        let final_cell_update = v_weighted_t.matmul(&k.contiguous()?)?; // [B, H, D, D] 

        let mut final_cell = (final_cell_initial + final_cell_update)?; 

        // Soft-normalización dinámica de la matriz de memoria
        let abs_cell = final_cell.abs()?;
        let c_max_cols = abs_cell.max(3)?;
        let c_max_rows = c_max_cols.max(2)?;
        
        let ten = Tensor::new(10.0f32, device)?;
        let denom = c_max_rows.broadcast_add(&ten)?; 
        let scale_factor = ten.broadcast_div(&denom)?;
        
        let scale_factor = scale_factor.unsqueeze(2)?.unsqueeze(3)?;
        final_cell = final_cell.broadcast_mul(&scale_factor)?;
 
        let final_hidden = h_seq.narrow(1, last_idx, 1)?.reshape((batch_size, self.hidden_size))?; 
 
        Ok((h_seq, MLstmstate::new(final_cell, final_hidden, final_norm, final_m)))
    }
}