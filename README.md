[ Versi贸n en Espa帽ol](#cortex-xor-sala-de-pruebas-experimental-de-rnns)

# Cortex-XOR: Experimental RNN Playground

This repository serves as an experimental testing ground for advanced Recurrent Neural Network (RNN) architectures implemented in Rust using the Candle framework. It is currently a work in progress and does not represent a final, concrete product.

## Core Components Tested
We are actively implementing and testing the following blocks, often in stacked configurations:

*   **sLSTM (Scalar LSTM):** Features exponential gating to improve scalar memory capabilities.
*   **mLSTM (Matrix LSTM):** Utilizes a matrix memory state to significantly enhance storage capacity.
*   **minGRU:** A minimal, efficient Gated Recurrent Unit.
*   **minLSTM:** A simplified and optimized variant of the traditional LSTM.

## Key Features & Goals
*   **Stacked Architectures:** We are experimenting with deep networks by stacking these blocks in various patterns (e.g., alternating sLSTM and mLSTM layers).
*   **Parallel Execution:** We are developing parallelized versions of these blocks to maximize performance and training efficiency.
*   **Paper Alignment:** The implementation aims to follow the original "xLSTM: Extended Long Short-Term Memory" (Beck et al., 2024) and related research papers very closely, though exact adherence is sometimes maintained flexibly for experimental purposes and practical optimization.

## Training & Methodology
*   **Datasets:** The models are primarily trained and benchmarked on **Shakespeare** and **TinyStories V2** datasets.
*   **Process:** The project is in a state of constant flux, with ongoing stability tests, parameter sweeps, and continuous code improvements.

---

# Cortex-XOR: Sala de Pruebas Experimental de RNNs

Este repositorio sirve como una sala de pruebas experimental para arquitecturas avanzadas de Redes Neuronales Recurrentes (RNN) implementadas en Rust utilizando el framework Candle. Actualmente es un trabajo en progreso y no representa un producto final o concreto.

## Componentes Principales Probados
Estamos implementando y probando activamente los siguientes bloques, a menudo en configuraciones apiladas:

*   **sLSTM (Scalar LSTM):** Incorpora "gating" (compuertas) exponencial para mejorar la capacidad de memoria escalar.
*   **mLSTM (Matrix LSTM):** Utiliza un estado de memoria matricial para aumentar significativamente la capacidad de almacenamiento.
*   **minGRU:** Una Unidad Recurrente con Compuertas (GRU) minimalista y eficiente.
*   **minLSTM:** Una variante simplificada y optimizada del LSTM tradicional.

## Caracter铆sticas Clave y Objetivos
*   **Arquitecturas Apiladas:** Estamos experimentando con redes profundas apilando estos bloques en varios patrones (por ejemplo, alternando capas de sLSTM y mLSTM).
*   **Ejecuci贸n Paralela:** Estamos desarrollando versiones paralizadas de estos bloques para maximizar el rendimiento y la eficiencia del entrenamiento.
*   **Alineaci贸n con el Paper:** La implementaci贸n intenta seguir muy de cerca el paper original "xLSTM: Extended Long Short-Term Memory" (Beck et al., 2024) y las investigaciones relacionadas, aunque no es exacto al 100%, permitiendo flexibilidad para la experimentaci贸n y optimizaci贸n pr谩ctica.

## Entrenamiento y Metodolog铆a
*   **Datasets:** Los modelos se entrenan y eval煤an principalmente con los datasets de **Shakespeare** y **TinyStories V2**.
*   **Proceso:** El proyecto est谩 en un estado de cambio constante, con pruebas de estabilidad en curso, barridos de par谩metros y mejoras continuas en el c贸digo.
